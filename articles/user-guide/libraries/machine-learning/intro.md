---
title: Biblioteca de aprendizaje automático cuántico
description: Obtenga información sobre cómo usar machine learning en sistemas Quantum
author: alexeib2
ms.author: alexeib
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: 9f7f892fb2b76432942c86163497c22f0c73d51f
ms.sourcegitcommit: 9b0d1ffc8752334bd6145457a826505cc31fa27a
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 09/21/2020
ms.locfileid: "90833811"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="00381-103">Introducción a Quantum Machine Learning</span><span class="sxs-lookup"><span data-stu-id="00381-103">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="00381-104">Marco de trabajo y objetivos</span><span class="sxs-lookup"><span data-stu-id="00381-104">Framework and goals</span></span>

<span data-ttu-id="00381-105">La codificación Quantum y el procesamiento de la información son una alternativa eficaz a los clasificadores de Quantum de aprendizaje automático clásico.</span><span class="sxs-lookup"><span data-stu-id="00381-105">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="00381-106">En concreto, nos permite codificar los datos de registros Quantum que son concisos en relación con el número de características, con lo que se emplea sistemáticamente el nivel de Quantum como recurso computacional y se emplea la medida Quantum para la inferencia de clases.</span><span class="sxs-lookup"><span data-stu-id="00381-106">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="00381-107">El clasificador de Quantum centrados en circuitos es una solución Quantum relativamente sencilla que combina la codificación de datos con un circuito de Quantum de inenredo/disentangling rápido seguido de la medida para deducir las etiquetas de clase de las muestras de datos.</span><span class="sxs-lookup"><span data-stu-id="00381-107">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="00381-108">El objetivo es garantizar la caracterización clásica y el almacenamiento de los circuitos de asunto, así como el entrenamiento híbrido/clásico de los parámetros del circuito, incluso en el caso de los espacios de características muy grandes.</span><span class="sxs-lookup"><span data-stu-id="00381-108">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="00381-109">Arquitectura de clasificador</span><span class="sxs-lookup"><span data-stu-id="00381-109">Classifier architecture</span></span>

<span data-ttu-id="00381-110">La clasificación es una tarea de aprendizaje automático supervisada, en la que el objetivo es deducir las etiquetas de clase $ \{ Y_1, y_2, \ldots y_d \} $ de ciertas muestras de datos.</span><span class="sxs-lookup"><span data-stu-id="00381-110">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="00381-111">El "conjunto de datos de entrenamiento" es una colección de ejemplos $ \mathcal{D} = \{ (x, y)} $ con etiquetas preasignadas conocidas.</span><span class="sxs-lookup"><span data-stu-id="00381-111">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="00381-112">Aquí $x $ es una muestra de datos y $y $ es su etiqueta conocida denominada "etiqueta de entrenamiento".</span><span class="sxs-lookup"><span data-stu-id="00381-112">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="00381-113">En cierto modo similares a los métodos tradicionales, la clasificación Quantum consta de tres pasos:</span><span class="sxs-lookup"><span data-stu-id="00381-113">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="00381-114">codificación de datos</span><span class="sxs-lookup"><span data-stu-id="00381-114">data encoding</span></span>
- <span data-ttu-id="00381-115">preparación de un estado de clasificador</span><span class="sxs-lookup"><span data-stu-id="00381-115">preparation of a classifier state</span></span>
- <span data-ttu-id="00381-116">medida debido a la naturaleza probabilística de la medición, estos tres pasos deben repetirse varias veces.</span><span class="sxs-lookup"><span data-stu-id="00381-116">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="00381-117">Tanto la codificación como la informática del estado de clasificador se realizan por medio de *circuitos Quantum*.</span><span class="sxs-lookup"><span data-stu-id="00381-117">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="00381-118">Aunque el circuito de codificación está normalmente controlado por datos y sin parámetros, el circuito clasificador contiene un conjunto suficiente de parámetros que se pueden aprender.</span><span class="sxs-lookup"><span data-stu-id="00381-118">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="00381-119">En la solución propuesta, el circuito clasificador se compone de rotaciones de un solo qubit y rotaciones controladas por dos qubit.</span><span class="sxs-lookup"><span data-stu-id="00381-119">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="00381-120">Los parámetros que se pueden aprender aquí son los ángulos de rotación.</span><span class="sxs-lookup"><span data-stu-id="00381-120">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="00381-121">Se sabe que las puertas de rotación y rotación controlada son *universales* para el cálculo de Quantum, lo que significa que cualquier matriz de peso unitario se puede descomponer en un circuito suficientemente largo que conste de dichas puertas.</span><span class="sxs-lookup"><span data-stu-id="00381-121">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="00381-122">En la versión propuesta, solo se admite un circuito seguido de una estimación de una sola frecuencia.</span><span class="sxs-lookup"><span data-stu-id="00381-122">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="00381-123">Por lo tanto, la solución es un Quantum análogo de una máquina de vectores de soporte con un kernel Polinómico de bajo nivel.</span><span class="sxs-lookup"><span data-stu-id="00381-123">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![Perceptrón multinivel frente a clasificador centrado en circuito](~/media/DLvsQCC.png)

<span data-ttu-id="00381-125">Un sencillo diseño de clasificadores de quantums puede compararse con una solución de máquina de vectores de soporte (SVM) tradicional.</span><span class="sxs-lookup"><span data-stu-id="00381-125">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="00381-126">La inferencia de una muestra de datos $x $ en el caso de SVM se realiza con un formato de kernel óptimo $ \sum \ alpha_j k (x_j, x) $, donde $k $ es una determinada función de kernel.</span><span class="sxs-lookup"><span data-stu-id="00381-126">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="00381-127">Por el contrario, un clasificador de Quantum usa el $p de predicción (y │ x, U (\theta)) = 〈 U (\theta) x | M | U (\theta) x 〉 $, que es similar en el espíritu pero técnicamente bastante diferente.</span><span class="sxs-lookup"><span data-stu-id="00381-127">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="00381-128">Por lo tanto, cuando se utiliza una codificación de amplitud sencilla, $p (y │ x, U (\theta)) $ es una forma cuadrática en las amplitudes de $x $, pero los coeficientes de este formulario ya no se aprenden de forma independiente; en su lugar, se agregan a partir de los elementos de la matriz del $U del circuito (\theta) $, que normalmente tiene significativamente menos parámetros que se pueden aprender $ \theta $ que la dimensión del vector $x $.</span><span class="sxs-lookup"><span data-stu-id="00381-128">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="00381-129">El grado Polinómico de $p (y │ x, U (\theta)) $ en las características originales se puede aumentar a $2 ^ l $ mediante el uso de una codificación de producto Quantum en $l $ copias de $x $.</span><span class="sxs-lookup"><span data-stu-id="00381-129">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="00381-130">Nuestra arquitectura explora circuitos relativamente superficiales que, por tanto, deben estar en *rápida* para capturar todas las correlaciones entre las características de datos en todos los intervalos.</span><span class="sxs-lookup"><span data-stu-id="00381-130">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="00381-131">En la ilustración siguiente se muestra un ejemplo del componente de circuito de inestabilidad de rápida utilidad.</span><span class="sxs-lookup"><span data-stu-id="00381-131">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="00381-132">Aunque un circuito con esta geometría consta solo de $3 n + 1 $ portones, la matriz de peso unitario que calcula garantiza una comunicación cruzada significativa entre las características de $2 ^ n $.</span><span class="sxs-lookup"><span data-stu-id="00381-132">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![Golpee rápidamente el circuito Quantum en 5 qubits (con dos capas cíclicas).](~/media/5-qubit-qccc.png)

<span data-ttu-id="00381-134">El circuito del ejemplo anterior consta de 6 puertas de qubit única $ (G_1, \ldots G_5; G_ {16} ) $ y 10 2-qubits Gates $ (G_6, \ldots, G_ {15} ) $.</span><span class="sxs-lookup"><span data-stu-id="00381-134">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="00381-135">Suponiendo que cada una de las puertas se define con un parámetro más aprendido, tenemos 16 parámetros que se van a aprender, mientras que la dimensión del espacio Hilbert de 5-qubit es 32.</span><span class="sxs-lookup"><span data-stu-id="00381-135">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="00381-136">Esta geometría de circuito se puede generalizar fácilmente a cualquier $n registro $-qubit, cuando $n $ es impar, produciendo circuitos con $3 n + 1 $ Parameters para el espacio de características $2 ^ n $-dimensional.</span><span class="sxs-lookup"><span data-stu-id="00381-136">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="00381-137">Aprendizaje clasificador como una tarea de aprendizaje supervisado</span><span class="sxs-lookup"><span data-stu-id="00381-137">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="00381-138">El entrenamiento de un modelo clasificador implica buscar valores óptimos de sus parámetros operativos, de modo que maximicen la probabilidad media de deducir las etiquetas de entrenamiento correctas en los ejemplos de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="00381-138">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="00381-139">Aquí nos referimos a nosotros solo con la clasificación de dos niveles, es decir, el caso de $d = $2 y solo dos clases con las etiquetas $y _ 1, y_2 $.</span><span class="sxs-lookup"><span data-stu-id="00381-139">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="00381-140">Una manera de generalizar nuestros métodos a un número arbitrario de clases es reemplazar qubits con qudits, es decir, las unidades de Quantum con Estados de $d $ Basis y la medición bidireccional con $d medida de $-Way.</span><span class="sxs-lookup"><span data-stu-id="00381-140">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="00381-141">Probabilidad como objetivo de entrenamiento</span><span class="sxs-lookup"><span data-stu-id="00381-141">Likelihood as the training goal</span></span>

<span data-ttu-id="00381-142">Dado un circuito de Quantum más aprendido $U (\theta) $, donde $ \theta $ es un vector de parámetros y que denota la medida final de $M $, la probabilidad media de la inferencia de etiqueta correcta es $ $ \begin{align} \mathcal{L} (\theta) = \frac {1} {| \mathcal{D} |} \left (\ sum_ {(x, y_1) \In\mathcal{D}} P (M = Y_1 | U (\theta) x) + \ sum_ {(x y_2) \in\mathcal{D}} P (M = y_2 | U (\theta) x) \right) \end{align} $ $ Where $P (M = y | z) $ es la probabilidad de medir $y $ en el estado de Quantum $z $.</span><span class="sxs-lookup"><span data-stu-id="00381-142">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="00381-143">En este caso, basta con comprender que la función de probabilidad $ \mathcal{L} (\theta) $ es fluida en $ \theta $ y su derivado en cualquier $ \ theta_j $ se puede calcular esencialmente en el mismo protocolo Quantum que se usa para calcular la función de probabilidad.</span><span class="sxs-lookup"><span data-stu-id="00381-143">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="00381-144">Esto permite optimizar $ \mathcal{L} (\theta) $ por descenso de gradiente.</span><span class="sxs-lookup"><span data-stu-id="00381-144">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="00381-145">Puntuación del clasificador y puntuación de entrenamiento</span><span class="sxs-lookup"><span data-stu-id="00381-145">Classifier bias and training score</span></span>

<span data-ttu-id="00381-146">Dados algunos valores intermedios (o finales) de los parámetros en $ \theta $, es necesario identificar un único valor real $b $ know como clasificación del *clasificador* para realizar la inferencia.</span><span class="sxs-lookup"><span data-stu-id="00381-146">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="00381-147">La regla de inferencia de etiqueta funciona de la siguiente manera:</span><span class="sxs-lookup"><span data-stu-id="00381-147">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="00381-148">En un ejemplo $x $ se le asigna la etiqueta $y _2 $ si y solo si $P (M = y_2 | U (\theta) x) + b > $0,5 (RULE1) (de lo contrario, se le asigna etiqueta $y _ 1 $)</span><span class="sxs-lookup"><span data-stu-id="00381-148">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="00381-149">Claramente $b $ debe estar en el intervalo $ (-0.5, + 0.5) $ para que sea significativo.</span><span class="sxs-lookup"><span data-stu-id="00381-149">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="00381-150">Un caso de entrenamiento $ (x, y) \en \mathcal{D} $ se considera una *clasificación* incorrecta dada la diferencia $b $ si la etiqueta deducida para $x $ como por RULE1 es realmente diferente de $y $.</span><span class="sxs-lookup"><span data-stu-id="00381-150">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="00381-151">El número total de clasificaciones incorrectas es la *puntuación de entrenamiento* del clasificador dada la diferencia $b $.</span><span class="sxs-lookup"><span data-stu-id="00381-151">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="00381-152">La diferencia de clasificador *óptima* $b $ minimiza la puntuación de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="00381-152">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="00381-153">Es fácil ver eso, según las estimaciones de probabilidad precalculadas $ \{ P (M = y_2 | U (\theta) x) | (x, \*) \in\mathcal{D} \} $, la diferencia de clasificador óptima se puede encontrar mediante la búsqueda binaria en el intervalo $ (-0.5, + 0.5) $ mediante la realización de un máximo de $ \ log_2 (| \mathcal{D} |) $ Steps.</span><span class="sxs-lookup"><span data-stu-id="00381-153">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="00381-154">Referencia</span><span class="sxs-lookup"><span data-stu-id="00381-154">Reference</span></span>

<span data-ttu-id="00381-155">Esta información debe ser suficiente para empezar a reproducir con el código.</span><span class="sxs-lookup"><span data-stu-id="00381-155">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="00381-156">Sin embargo, si desea obtener más información sobre este modelo, lea la propuesta original: [ *' clasificadores de Quantum centrados en circuitos ', María Schuld, Alex Bocharov, KRYSTA Svore y Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="00381-156">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="00381-157">Además del ejemplo de código que verá en los pasos siguientes, también puede empezar a explorar la clasificación de Quantum en [este tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification) .</span><span class="sxs-lookup"><span data-stu-id="00381-157">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/main/tutorials/QuantumClassification)</span></span> 
