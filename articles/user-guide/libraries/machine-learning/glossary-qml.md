---
title: Biblioteca de aprendizaje automático cuántico
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: 52c3f69fb99384270a27e57c4f32212d18bee1a4
ms.sourcegitcommit: 6bf99d93590d6aa80490e88f2fd74dbbee8e0371
ms.translationtype: MT
ms.contentlocale: es-ES
ms.lasthandoff: 08/06/2020
ms.locfileid: "87868906"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="e72a5-102">Glosario de Machine Learning Quantum</span><span class="sxs-lookup"><span data-stu-id="e72a5-102">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="e72a5-103">El entrenamiento de un clasificador de Quantum centrado en circuitos es un proceso con muchas partes móviles que requieren la misma cantidad de calibrado (o un poco mayor) de calibración por prueba y error como formación de clasificadores tradicionales.</span><span class="sxs-lookup"><span data-stu-id="e72a5-103">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="e72a5-104">Aquí se definen los principales conceptos e ingredientes de este proceso de entrenamiento.</span><span class="sxs-lookup"><span data-stu-id="e72a5-104">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="e72a5-105">Programaciones de entrenamiento y pruebas</span><span class="sxs-lookup"><span data-stu-id="e72a5-105">Training/testing schedules</span></span>

<span data-ttu-id="e72a5-106">En el contexto del entrenamiento del clasificador, una *programación* describe un subconjunto de muestras de datos en un conjunto de entrenamiento o de prueba global.</span><span class="sxs-lookup"><span data-stu-id="e72a5-106">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="e72a5-107">Normalmente, una programación se define como una colección de índices de ejemplo.</span><span class="sxs-lookup"><span data-stu-id="e72a5-107">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="e72a5-108">Puntuaciones de parámetros/sesgo</span><span class="sxs-lookup"><span data-stu-id="e72a5-108">Parameter/bias scores</span></span>

<span data-ttu-id="e72a5-109">Dado un vector de parámetro candidato y una diferencia del clasificador, su *puntuación de validación* se mide en relación con las programaciones de validación seleccionadas y se expresa mediante un número de clasificaciones incorrectas contadas en todos los ejemplos de las programaciones.</span><span class="sxs-lookup"><span data-stu-id="e72a5-109">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="e72a5-110">Hiperparámetros</span><span class="sxs-lookup"><span data-stu-id="e72a5-110">Hyperparameters</span></span>

<span data-ttu-id="e72a5-111">El proceso de entrenamiento del modelo se rige por ciertos valores preestablecidos denominados *hiperparámetros*:</span><span class="sxs-lookup"><span data-stu-id="e72a5-111">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="e72a5-112">Velocidad de aprendizaje</span><span class="sxs-lookup"><span data-stu-id="e72a5-112">Learning rate</span></span>

<span data-ttu-id="e72a5-113">Es uno de los hiperparámetros de clave.</span><span class="sxs-lookup"><span data-stu-id="e72a5-113">It is one of the key hyperparameters.</span></span> <span data-ttu-id="e72a5-114">Define cuánta estimación de degradado de estocástico actual afecta a la actualización del parámetro.</span><span class="sxs-lookup"><span data-stu-id="e72a5-114">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="e72a5-115">El tamaño del delta de actualización de parámetros es proporcional a la velocidad de aprendizaje.</span><span class="sxs-lookup"><span data-stu-id="e72a5-115">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="e72a5-116">Los valores de velocidad de aprendizaje más pequeños conducen a una evolución de parámetros más lenta y una convergencia más lenta, pero los valores excesivamente grandes de LR pueden romper la convergencia como el descenso del degradado nunca se confirma en un mínimo local determinado.</span><span class="sxs-lookup"><span data-stu-id="e72a5-116">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="e72a5-117">Aunque el algoritmo de entrenamiento ajusta de forma adaptable la velocidad de aprendizaje en cierta medida, es importante seleccionar un buen valor inicial.</span><span class="sxs-lookup"><span data-stu-id="e72a5-117">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="e72a5-118">Un valor inicial predeterminado habitual para la velocidad de aprendizaje es 0,1.</span><span class="sxs-lookup"><span data-stu-id="e72a5-118">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="e72a5-119">La selección del mejor valor de velocidad de aprendizaje es una buena idea (vea, por ejemplo, la sección 4,3 de Goodfellow et al., "aprendizaje profundo", MIT Press, 2017).</span><span class="sxs-lookup"><span data-stu-id="e72a5-119">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="e72a5-120">Tamaño de minilote</span><span class="sxs-lookup"><span data-stu-id="e72a5-120">Minibatch size</span></span>

<span data-ttu-id="e72a5-121">Define el número de muestras de datos que se usan para una única estimación del degradado estocástico.</span><span class="sxs-lookup"><span data-stu-id="e72a5-121">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="e72a5-122">Los valores más grandes del tamaño de minilote generalmente conducen a una convergencia más sólida y más monotónica, pero pueden ralentizar el proceso de entrenamiento, ya que el costo de una estimación de degradado es proporcional al tamaño de MiniMatch.</span><span class="sxs-lookup"><span data-stu-id="e72a5-122">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="e72a5-123">Un valor predeterminado habitual para el tamaño de minilote es 10.</span><span class="sxs-lookup"><span data-stu-id="e72a5-123">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="e72a5-124">Tiempo de entrenamiento, tolerancia, gridlocks</span><span class="sxs-lookup"><span data-stu-id="e72a5-124">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="e72a5-125">"Tiempo" hace referencia a un paso completo a través de los datos de entrenamiento programados.</span><span class="sxs-lookup"><span data-stu-id="e72a5-125">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="e72a5-126">El número máximo de tiempo por un subproceso de entrenamiento (consulte a continuación) debe estar limitado.</span><span class="sxs-lookup"><span data-stu-id="e72a5-126">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="e72a5-127">El subproceso de entrenamiento se define para finalizar (con los mejores parámetros de candidato conocidos) cuando se ha ejecutado el número máximo de tiempos.</span><span class="sxs-lookup"><span data-stu-id="e72a5-127">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been executed.</span></span> <span data-ttu-id="e72a5-128">Sin embargo, este entrenamiento terminaría antes cuando la tasa de errores de clasificación en la programación de la validación cae por debajo de una tolerancia seleccionada.</span><span class="sxs-lookup"><span data-stu-id="e72a5-128">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="e72a5-129">Supongamos, por ejemplo, que la tolerancia de clasificación incorrecta es 0,01 (1%); si en el conjunto de validación de 2000 ejemplos vemos menos de 20 clasificaciones incorrectas, se ha logrado el nivel de tolerancia.</span><span class="sxs-lookup"><span data-stu-id="e72a5-129">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="e72a5-130">Un subproceso de entrenamiento también finaliza prematuramente si la puntuación de validación del modelo candidato no ha mostrado ninguna mejora respecto a varias duraciones consecutivas (un Gridlock).</span><span class="sxs-lookup"><span data-stu-id="e72a5-130">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="e72a5-131">La lógica para la terminación de Gridlock está codificada actualmente.</span><span class="sxs-lookup"><span data-stu-id="e72a5-131">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="e72a5-132">Recuento de medidas</span><span class="sxs-lookup"><span data-stu-id="e72a5-132">Measurements count</span></span>

<span data-ttu-id="e72a5-133">La estimación de las puntuaciones de entrenamiento y validación y los componentes del degradado estocástico en un dispositivo Quantum se calcula para calcular las superposiciones de estado de Quantum que requieren varias mediciones del observables adecuado.</span><span class="sxs-lookup"><span data-stu-id="e72a5-133">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="e72a5-134">El número de medidas se debe escalar como $O (1/\ épsilon ^ 2) $ donde $ \epsilon $ es el error de estimación deseado.</span><span class="sxs-lookup"><span data-stu-id="e72a5-134">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="e72a5-135">Como regla general, el número de mediciones iniciales podría ser aproximadamente $1/\ mbox {Tolerance} ^ 2 $ (consulte la definición de tolerancia en el párrafo anterior).</span><span class="sxs-lookup"><span data-stu-id="e72a5-135">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="e72a5-136">Es necesario revisar el recuento de medidas hacia arriba si el descenso del degradado parece ser demasiado errático y convergencia demasiado difícil de lograr.</span><span class="sxs-lookup"><span data-stu-id="e72a5-136">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="e72a5-137">Entrenamiento de subprocesos</span><span class="sxs-lookup"><span data-stu-id="e72a5-137">Training threads</span></span>

<span data-ttu-id="e72a5-138">La función de probabilidad, que es la utilidad de entrenamiento del clasificador, es muy rara vez convexa, lo que significa que normalmente tiene una gran cantidad de optimación local en el espacio de parámetros que puede diferir significativamente en la calidad.</span><span class="sxs-lookup"><span data-stu-id="e72a5-138">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="e72a5-139">Dado que el proceso SGD puede converger solo en un óptimo específico, es importante explorar varios vectores de parámetro de inicio.</span><span class="sxs-lookup"><span data-stu-id="e72a5-139">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="e72a5-140">Una práctica común en el aprendizaje automático es inicializar los vectores iniciales de forma aleatoria.</span><span class="sxs-lookup"><span data-stu-id="e72a5-140">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="e72a5-141">La Q# API de entrenamiento acepta una matriz arbitraria de tales vectores de inicio, pero el código subyacente los explora secuencialmente.</span><span class="sxs-lookup"><span data-stu-id="e72a5-141">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="e72a5-142">En un equipo multinúcleo o de hecho en cualquier arquitectura de computación en paralelo, es aconsejable realizar varias llamadas a Q# la API de entrenamiento en paralelo con inicializaciones de parámetros diferentes en las llamadas.</span><span class="sxs-lookup"><span data-stu-id="e72a5-142">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="e72a5-143">Cómo modificar los hiperparámetros</span><span class="sxs-lookup"><span data-stu-id="e72a5-143">How to modify the hyperparameters</span></span>

<span data-ttu-id="e72a5-144">En la biblioteca QML, la mejor manera de modificar los hiperparámetros es invalidar los valores predeterminados del UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions) .</span><span class="sxs-lookup"><span data-stu-id="e72a5-144">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions).</span></span> <span data-ttu-id="e72a5-145">Para ello, se llama con la función [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) y se aplica el operador `w/` para invalidar los valores predeterminados.</span><span class="sxs-lookup"><span data-stu-id="e72a5-145">To do this we call it with the function [`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="e72a5-146">Por ejemplo, para usar medidas 100.000 y una velocidad de aprendizaje de 0,01:</span><span class="sxs-lookup"><span data-stu-id="e72a5-146">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
